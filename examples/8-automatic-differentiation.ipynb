{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15cbaf9a-60d8-4998-97c2-fb013581470d",
   "metadata": {},
   "source": [
    "### An example of Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2419cc7e-8c74-4019-9f9f-7dd76534e8f4",
   "metadata": {},
   "source": [
    "Pauli propagation has promise in optimizing variational models because its elementary operations are very simple and automatically differentiable. Additionally, it may be the case that strongly truncated simulations still yield final parameters that translate to high-quality quantum circuits on, for example, a quantum computer. This means that we could (perhaps) very cheaply train/optimize, and then deploy the learned model on a different classical or quantum method. There are some initial indications of this being possible, but they require further exploration. \n",
    "\n",
    "In this notebook, we demonstrate how to calculate fast automatic gradients of a quantum circuit using Pauli propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20669a1a-a02c-43d9-ab41-f83aafd83ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "using PauliPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57585029-6b3d-40c4-871f-28900a04193f",
   "metadata": {},
   "source": [
    "Let us set up a simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707c3ac9-d46b-4c55-9607-2fcff94de271",
   "metadata": {},
   "outputs": [],
   "source": [
    "nq = 16\n",
    "\n",
    "topology = bricklayertopology(nq);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a24c7c3-e326-4563-91dd-ce9044ee8765",
   "metadata": {},
   "source": [
    "We define a transverse field Hamiltonian, whose derivative we will compute. This could be used within a variational energy minimization routine to find its ground state. \n",
    "\n",
    "The Hamiltonian here reads $H = \\sum_{i}X_i + \\sum_{\\langle i, j\\rangle}Z_iZ_j$ where $ \\langle i, j\\rangle$ denotes neighbors on the topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1d5fb1-5cc0-419f-a50a-5126a5c20e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PauliSum(nqubits: 16, 31 Pauli terms:\n",
       " 1.0 * IIXIIIIIIIIIIIII\n",
       " 1.0 * IIIIIIIIIIIIXIII\n",
       " 1.0 * IIIIIIIIIIIIIIIX\n",
       " 1.0 * IZZIIIIIIIIIIIII\n",
       " 1.0 * IIIIIIIIIIIIIXII\n",
       " 1.0 * IIIIIIXIIIIIIIII\n",
       " 1.0 * IIIIIIIIZZIIIIII\n",
       " 1.0 * XIIIIIIIIIIIIIII\n",
       " 1.0 * IIIIXIIIIIIIIIII\n",
       " 1.0 * IIIIIIIIIIIIZZII\n",
       " 1.0 * IIIIZZIIIIIIIIII\n",
       " 1.0 * IIIIIZZIIIIIIIII\n",
       " 1.0 * IIIIIIIZZIIIIIII\n",
       " 1.0 * IIIIIXIIIIIIIIII\n",
       " 1.0 * IIIIIIIIIIIZZIII\n",
       " 1.0 * IIIIIIIIIIXIIIII\n",
       " 1.0 * IIIIIIIIIZZIIIII\n",
       " 1.0 * IIIZZIIIIIIIIIII\n",
       " 1.0 * IIIIIIIIIIIXIIII\n",
       " 1.0 * IIIXIIIIIIIIIIII\n",
       "  â‹®)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = PauliSum(nq)\n",
    "\n",
    "for qind in 1:nq\n",
    "    add!(H, :X, qind, 1.0)\n",
    "end\n",
    "\n",
    "for pair in topology\n",
    "    add!(H, [:Z, :Z], collect(pair), 1.0)\n",
    "end\n",
    "\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5e402-e941-4f2f-8f3e-2d0707c91e5d",
   "metadata": {},
   "source": [
    "Define some generic quantum circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12381cf7-77ff-481a-9098-518b71d34217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl = 4\n",
    "\n",
    "circuit = hardwareefficientcircuit(nq, nl; topology=topology)\n",
    "nparams = countparameters(circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed752b6-0b10-4d0a-9a24-e0129edf1961",
   "metadata": {},
   "source": [
    "Importantly, we need to set our truncations. Depending on which package and which method you are using to compute your gradients, you can use different truncations. \n",
    "\n",
    "`ReverseDiff` for example is a sophisticated package for automatic _reverse-mode_ differentiation (others may or may not work better). It will build a computational graph that it then differentiates using the chain rule. This is how large-scale neural networks are trained, and is commonly referred to as gradient _backpropagation_. The challenge here is that the fastest gradients are achieved with a _compiled_ version of the gradient function where the graph for the chain rule is computed once (to the best of our knowledge). In that case, only truncations during the initial computation will be respected. Truncations that we think work well here are `max_weight`, `max_freq`, and `max_sins`, as they do not depend on the particular parameters of the quantum circuit. On the other hand, which paths are explore with truncations such as `min_abs_coeff` will not be updated (again, to the best of our knowledge) as the gradients are computed. If you want to use such parameter-dependent truncations, you may need to use un-compiled gradient functions.\n",
    "\n",
    "Packages such as `ForwardDiff` or manual numerical differentiation, on the other hand, always involve computation of the loss function, which is affected by all truncations. Unfortunately, these methods are slower for circuits with more than several dozen parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f55eeed-8972-4be0-9585-49733d70f138",
   "metadata": {},
   "source": [
    "Generate some generic parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c63006-5d77-43ec-af6c-64b3a7e24212",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "Random.seed!(42)\n",
    "thetas = randn(nparams);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d3caa-30c5-431f-b3fd-0ebf2548477c",
   "metadata": {},
   "source": [
    "One expectation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bef1ff0-7fd3-4b78-8f54-14ee1f660140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.833596 seconds (1.08 M allocations: 58.479 MiB, 85.88% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.8904666076774599"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_freq = 25\n",
    "max_weight = 5\n",
    "\n",
    "@time psum = propagate(circuit, H, thetas; max_freq, max_weight);\n",
    "overlapwithzero(psum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b54e25-e5cb-4a8d-ac26-f02d849b2249",
   "metadata": {},
   "source": [
    "**Note**: To make these following functionsfaster, you should look into `let` blocks for local variable namespaces. You will want to define _closures_ and refrain from using global variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec61b35-cf4c-459f-9cdc-d0ab48689011",
   "metadata": {},
   "source": [
    "#### What does not work:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be8e1dc-c677-413e-a1c1-c34ba143ab58",
   "metadata": {},
   "source": [
    "Now wrap these steps into a function that takes only `thetas` as argument. (keep in mind our comment about using `let` blocks)\n",
    "\n",
    "This loss function does not work because the `ReverseDiff` package needs to propagate its custom coefficient type. But `H` is already strictly typed. So the following loss function would not be automatically differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff42aa41-fed2-42a3-974e-6c28861bcb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "naivelossfunction (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function naivelossfunction(thetas)   \n",
    "    psum = propagate(circuit, H, thetas; max_freq, max_weight);\n",
    "    return overlapwithzero(psum)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bfcf6c-e749-4421-a78e-01dcddd75af6",
   "metadata": {},
   "source": [
    "The loss works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f111331a-7405-4191-811f-cd5d26d88b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.271859 seconds (1.22 M allocations: 67.234 MiB, 3.01% gc time, 57.10% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.8904666076774599"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time naivelossfunction(thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15507e9d-8eea-4a71-8547-daf0b931afbb",
   "metadata": {},
   "source": [
    "But the gradient would break with a cryptic error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53158f29-cd6e-441b-a8ea-314ee633bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pkg; Pkg.add(\"ReverseDiff\")\n",
    "using ReverseDiff: gradient\n",
    "\n",
    "## this errors \n",
    "# gradient(naivelossfunction, thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34de0990-8fc3-4921-b45a-fc858bd0778b",
   "metadata": {},
   "source": [
    "#### What works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5658ef0c-20ea-4bce-9e26-147a92a61bfc",
   "metadata": {},
   "source": [
    "To create a loss function that indeed works, we build the Hamiltonian with the correct coefficient type within the loss function. What tends to work is to type the coefficients in the Pauli sum to the element type of `thetas`, but this will not always work. In this case, it will make everything differentiable.\n",
    "\n",
    "Another somewhat annoying thing we need to do is to manually convert the coefficients of the Hamiltonian into `PauliFreqTracker` objects. Normally, in `propagate()` we automatically convert coefficients to our `PauliFreqTracker` types when `max_freq` or `max_sin` are set. In the in-place version `propagate!()`, we intentionally do not convert automatically. But here we need to use the `propagate!()` function because otherwise the ReverseDiff library errors because we copy the Pauli sum while that library is building its differentiation graph. Annoying... we agree! We will keep an eye out for better options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df906060-d2e3-495f-b043-667da74181ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lossfunction (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function lossfunction(thetas)\n",
    "    # differentiation libraries use custom types to trace through the computation\n",
    "    # we need to make all of our objects typed like that so that nothing breaks\n",
    "    CoeffType = eltype(thetas)\n",
    "\n",
    "    # define H again \n",
    "    H = PauliSum(CoeffType, nq)\n",
    "    for qind in 1:nq\n",
    "        add!(H, :X, qind, CoeffType(1.0))\n",
    "    end\n",
    "    for pair in topology\n",
    "        add!(H, [:Z, :Z], collect(pair), CoeffType(1.0))\n",
    "    end\n",
    "\n",
    "    # wrapp the coefficients into PauliFreqTracker so that we can use `max_freq` truncation.\n",
    "    # usually this happens automatically but the in-place propagate!() function does not allow that.\n",
    "    wrapped_H = wrapcoefficients(H, PauliFreqTracker)\n",
    "\n",
    "    # be also need to run the in-place version with `!`, because by default we copy the Pauli sum\n",
    "    output_H = propagate!(circuit, wrapped_H, thetas; max_freq, max_weight);\n",
    "    return overlapwithzero(output_H)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669b874-094a-4570-8519-2bb15a7ae2ed",
   "metadata": {},
   "source": [
    "Instead, we need to define a loss function that creates H every time with the correct coefficient type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "666ca6a0-cc70-41f4-afb4-976a33813f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.233968 seconds (438.39 k allocations: 27.163 MiB, 50.53% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1.8904666076774599"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time lossfunction(thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b310aca8-77fe-4a8e-9b8b-7022e1350e5c",
   "metadata": {},
   "source": [
    "And gradients work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e17cbe3-e60e-48dc-86db-8c6dec773d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.714606 seconds (21.02 M allocations: 931.987 MiB, 8.48% gc time, 55.39% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "252-element Vector{Float64}:\n",
       " -0.5057064083446595\n",
       "  0.025028009253606664\n",
       " -0.47337187596655284\n",
       "  0.09285919084901251\n",
       "  0.0900343739787604\n",
       "  0.26751480771265024\n",
       "  0.07722722421907667\n",
       "  0.5186194278104864\n",
       " -0.4367624010611486\n",
       "  0.7767471716936789\n",
       "  0.0227644087188041\n",
       "  0.908680529040292\n",
       " -0.11929346203144106\n",
       "  â‹®\n",
       " -0.4835428738051231\n",
       " -0.2487625833982674\n",
       " -0.7705492109786312\n",
       "  0.2651823350777113\n",
       " -1.2311240255419973\n",
       "  1.4595405277023366\n",
       "  0.3972920774850484\n",
       " -0.7513152897757447\n",
       " -1.0433085832333235\n",
       "  0.6822884973623046\n",
       " -0.4331718877680776\n",
       " -0.24910861064112663"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time gradient(lossfunction, thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27d00a5-218c-420d-a510-7c46554ae0af",
   "metadata": {},
   "source": [
    "Now import ReverseDiff and follow their example for very fast gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcfde742-f364-4e88-8e4a-4811c39ab2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ReverseDiff: GradientTape, gradient!, compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37d103dd-a2f9-4547-91b6-6e563aff2fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.420120 seconds (17.09 M allocations: 739.430 MiB, 23.65% gc time, 0.53% compilation time)\n",
      "  0.696674 seconds (238.88 k allocations: 11.783 MiB, 25.98% compilation time)\n",
      "  0.522534 seconds\n"
     ]
    }
   ],
   "source": [
    "### This is following an ReverseDiff.jl example\n",
    "\n",
    "# some inputs and work buffer to play around with\n",
    "grad_array = similar(thetas);\n",
    "\n",
    "# pre-record a GradientTape for `gradsimulation` using inputs of length m with Float64 elements\n",
    "@time const simulation_tape = GradientTape(lossfunction, thetas)\n",
    "\n",
    "# first evaluation compiles and is slower\n",
    "@time gradient!(grad_array, simulation_tape, thetas)\n",
    "# second evaluation\n",
    "@time gradient!(grad_array, simulation_tape, thetas);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "789942a9-ad5a-4e77-b53d-768fd326a8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.427378 seconds (27.74 M allocations: 1.155 GiB, 7.41% gc time, 13.28% compilation time)\n",
      "  0.381263 seconds (74.84 k allocations: 3.675 MiB, 24.42% compilation time)\n",
      "  0.285837 seconds\n"
     ]
    }
   ],
   "source": [
    "# compile to make it even faster\n",
    "@time const compiled_simulation_tape = compile(simulation_tape)\n",
    "\n",
    "# some inputs and work buffer to play around with\n",
    "grad_array_compiled = similar(thetas);\n",
    "\n",
    "# first evaluation compiles and is slower\n",
    "@time gradient!(grad_array_compiled, compiled_simulation_tape, thetas)\n",
    "# second evaluation\n",
    "@time gradient!(grad_array_compiled, compiled_simulation_tape, thetas);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb32f06-9a78-44b5-854d-9992508bc8ea",
   "metadata": {},
   "source": [
    "`grad_array` here carries the gradient result. It is changed in-place in `gradient!` so that the array does not need to get allocated over and over. And the results between the compiled version and the non-compiled version is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ce18c4b-9614-4e12-af1b-28d579e127dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_array â‰ˆ grad_array_compiled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36437c88-7dc7-4384-bcf2-60c02d78638c",
   "metadata": {},
   "source": [
    "See how calculating the gradient is only a few times slower than calculating the loss! The magic if reverse-mode differentiation. Feel free to explore other automatic differentiation libraries, including ones using forward-mode differentiation for when you have very few parameters. Also keep in mind that the loss functions we have defined can be sped up by not either declaring the global variables as `const` or by passing them via so-called *closures*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

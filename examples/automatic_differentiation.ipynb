{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15cbaf9a-60d8-4998-97c2-fb013581470d",
   "metadata": {},
   "source": [
    "### An example of Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20669a1a-a02c-43d9-ab41-f83aafd83ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "using PauliPropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57585029-6b3d-40c4-871f-28900a04193f",
   "metadata": {},
   "source": [
    "Note that we will define a lot of variables going forward as constant via the `const` syntax. In Julia, this does not fix the value of the variable, but its type. This is vital when using global variables inside functions so that performance is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707c3ac9-d46b-4c55-9607-2fcff94de271",
   "metadata": {},
   "outputs": [],
   "source": [
    "const nq = 32\n",
    "\n",
    "const topology = bricklayertopology(nq);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a24c7c3-e326-4563-91dd-ce9044ee8765",
   "metadata": {},
   "source": [
    "We define a transverse field Hamiltonian, whose derivative we will compute. This could be used within a variational energy minimization routine to find its ground state. \n",
    "\n",
    "The Hamiltonian here reads $H = \\sum_{i}X_i + \\sum_{\\langle i, j\\rangle}Z_iZ_j$ where $ \\langle i, j\\rangle$ denotes neighbors on the topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1d5fb1-5cc0-419f-a50a-5126a5c20e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PauliSum(nqubits: 32, 63 Pauli terms:\n",
       " 1.0 * IZZIIIIIIIIIIIIIIIII...\n",
       " 1.0 * IIIIIIIIIIIIIXIIIIII...\n",
       " 1.0 * IIIIIXIIIIIIIIIIIIII...\n",
       " 1.0 * IIIIIIIIIIIIIIIIIIII...\n",
       " 1.0 * IIIIIIIIIIIIIIIIIIII...\n",
       " 1.0 * IIIIIIIIIIIIIIIIXIII...\n",
       " 1.0 * IIIXIIIIIIIIIIIIIIII...\n",
       " 1.0 * IIIIIIIIIIIIIIXIIIII...\n",
       " 1.0 * IIIIIIIXIIIIIIIIIIII...\n",
       " 1.0 * IXIIIIIIIIIIIIIIIIII...\n",
       " 1.0 * IIIIIIIIIIIIIZZIIIII...\n",
       " 1.0 * IIIIIIIIIIIIIIIIIIIX...\n",
       " 1.0 * IIIIIIXIIIIIIIIIIIII...\n",
       " 1.0 * IIIIIIIIZZIIIIIIIIII...\n",
       " 1.0 * IIIIXIIIIIIIIIIIIIII...\n",
       " 1.0 * IIIIIZZIIIIIIIIIIIII...\n",
       " 1.0 * IIIIIIIZZIIIIIIIIIII...\n",
       " 1.0 * IIIIIIIIIIIIIIIIIIXI...\n",
       " 1.0 * IIIIIIIIIIIIIIIIIIII...\n",
       " 1.0 * IIIIIIIIIIIIIIIIIIII...\n",
       "  ⋮)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "H = PauliSum(nq)\n",
    "\n",
    "for qind in 1:nq\n",
    "    add!(H, :X, qind, 1.0)\n",
    "end\n",
    "\n",
    "for pair in topology\n",
    "    add!(H, [:Z, :Z], pair, 1.0)\n",
    "end\n",
    "\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5e402-e941-4f2f-8f3e-2d0707c91e5d",
   "metadata": {},
   "source": [
    "Define some generic quantum circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12381cf7-77ff-481a-9098-518b71d34217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nl = 4\n",
    "\n",
    "# define our circuit and denote it with `const` to the code that uses this global variable fast\n",
    "const circuit = hardwareefficientcircuit(nq, nl; topology=topology)\n",
    "nparams = countparameters(circuit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed752b6-0b10-4d0a-9a24-e0129edf1961",
   "metadata": {},
   "source": [
    "Importantly, we need to set our truncations. Depending on which package you are using to compute your gradients, you can use different truncations. \n",
    "\n",
    "`ReverseDiff` for example is a sophisticated package for automatic _reverse-mode_ differentiation. It will build a computational graph that it then differentiates using the chain rule. This is how large-scale neural networks are trained, and is commonly referred to as gradient _backpropagation_. The challenge here is that the graph for the chain rule is computed once (to the best of our knowledge), which means that only truncations during the initial computation will be respected. Truncations that we think work well here are `max_weight`, `max_freq`, and `max_sins`, as they do not depend on the particular parameters of the quantum circuit. On the other hand, which paths are explore with truncations such as `min_abs_coeff` will not be updated (again, to the best of our knowledge) as the gradients are computed.\n",
    "\n",
    "Packages such as `ForwardDiff` or manual numerical differentiation, on the other hand, always involve computation of the loss function, which is affected by all truncations. Unfortunately, these methods are slower for circuits with more than several dozen parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a678897-e092-42a6-b4f7-8dba976a251c",
   "metadata": {},
   "source": [
    "So let's wrap the coefficients into `NumericPathProperties`, so we can truncate based on `max_freq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef0f532-71f3-490f-bd8e-0a5e0ceb2035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PauliSum(nqubits: 32, 63 Pauli terms:\n",
       " PathProperty(1.0) * IIIIIIIIIIIIIIIIIIII...\n",
       " PathProperty(1.0) * IIXIIIIIIIIIIIIIIIII...\n",
       " PathProperty(1.0) * IIIIIIIIIIIIIIIXIIII...\n",
       " PathProperty(1.0) * IZZIIIIIIIIIIIIIIIII...\n",
       " PathProperty(1.0) * IIIIIIIIIIIIIXIIIIII...\n",
       " PathProperty(1.0) * IIIIIIIIIIIIIIIIIIII...\n",
       " PathProperty(1.0) * IIIIIIIIIIIIIIIIIIII...\n",
       " PathProperty(1.0) * IIIIIIIIIIIIZZIIIIII...\n",
       " PathProperty(1.0) * IIIIIIIIIIIIIIIIIIII...\n",
       " PathProperty(1.0) * IIIIZZIIIIIIIIIIIIII...\n",
       " PathProperty(1.0) * IIIIIIIIIIIIIIIIIIII...\n",
       " PathProperty(1.0) * IIIIIXIIIIIIIIIIIIII...\n",
       " PathProperty(1.0) * IIIIIIIIIIIIIIIIIIII...\n",
       " PathProperty(1.0) * IIIIIIIIIIIIIIIIIIII...\n",
       " PathProperty(1.0) * IIIIIIIIIIIIIIIIXIII...\n",
       " PathProperty(1.0) * IIIIIIIIIIIIIIIZZIII...\n",
       " PathProperty(1.0) * IIIIIIIIIIIIIIIIIIII...\n",
       " PathProperty(1.0) * IIIXIIIIIIIIIIIIIIII...\n",
       " PathProperty(1.0) * IIIIIIZZIIIIIIIIIIII...\n",
       " PathProperty(1.0) * IIIIIIIIIIIIIIXIIIII...\n",
       "  ⋮)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const wrapped_H = wrapcoefficients(H, NumericPathProperties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262405b0-d3d8-430f-9a38-b366431fe91b",
   "metadata": {},
   "source": [
    "Define our truncations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f5ca715-490e-4e70-9d4f-3ddae523d1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "const max_freq = 30\n",
    "const max_weight = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f55eeed-8972-4be0-9585-49733d70f138",
   "metadata": {},
   "source": [
    "Generate some generic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36c63006-5d77-43ec-af6c-64b3a7e24212",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "Random.seed!(42)\n",
    "thetas = randn(nparams);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d3caa-30c5-431f-b3fd-0ebf2548477c",
   "metadata": {},
   "source": [
    "One expectation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bef1ff0-7fd3-4b78-8f54-14ee1f660140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.110080 seconds (626.27 k allocations: 61.114 MiB, 0.88% gc time, 46.09% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0578323811939663"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@time psum = propagate(circuit, wrapped_H, thetas; max_freq, max_weight);\n",
    "overlapwithzero(psum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be8e1dc-c677-413e-a1c1-c34ba143ab58",
   "metadata": {},
   "source": [
    "Now wrap it into a function that takes only `thetas` as argument. This is why we denoted many global variables as `const`, because we use them in here.\n",
    "\n",
    "This loss function does not work because the `ReverseDiff` package needs to propagate its custom coefficient type. But `H` is already stricktly typed. So the following loss function would not be automatically differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff42aa41-fed2-42a3-974e-6c28861bcb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "naivelossfunction (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function naivelossfunction(thetas)\n",
    "    psum = propagate(circuit, wrapped_H, thetas; max_freq, max_weight);\n",
    "    return overlapwithzero(psum)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f111331a-7405-4191-811f-cd5d26d88b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.609916 seconds (28.90 k allocations: 21.094 MiB, 1.40% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0578323811939663"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@time naivelossfunction(thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5658ef0c-20ea-4bce-9e26-147a92a61bfc",
   "metadata": {},
   "source": [
    "We now create a loss function that does indeed work. It requires that we build the Hamiltonian with the correct coefficient type, which here is the element type of `thetas`. This will make everything differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df906060-d2e3-495f-b043-667da74181ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lossfunction (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function lossfunction(thetas)\n",
    "    coefftype = eltype(thetas)\n",
    "\n",
    "    H = PauliSum(nq, coefftype)\n",
    "    for qind in 1:nq\n",
    "        add!(H, :X, qind, coefftype(1.0))\n",
    "    end\n",
    "    for pair in topology\n",
    "        add!(H, [:Z, :Z], pair, coefftype(1.0))\n",
    "    end\n",
    "    \n",
    "    wrapped_H = wrapcoefficients(H, NumericPathProperties)\n",
    "\n",
    "    # be also need to run the in-place version with `!`, because by default we copy the Pauli sum\n",
    "    wrapped_H = propagate!(circuit, wrapped_H, thetas; max_freq, max_weight);\n",
    "    return overlapwithzero(wrapped_H)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669b874-094a-4570-8519-2bb15a7ae2ed",
   "metadata": {},
   "source": [
    "Instead, we need to define a loss function that creates H every time with the correct coefficient type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "666ca6a0-cc70-41f4-afb4-976a33813f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.624830 seconds (57.79 k allocations: 23.024 MiB, 0.73% gc time, 5.27% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0578323811939663"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@time lossfunction(thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27d00a5-218c-420d-a510-7c46554ae0af",
   "metadata": {},
   "source": [
    "Now import ReverseDiff and follow their example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcfde742-f364-4e88-8e4a-4811c39ab2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ReverseDiff: GradientTape, gradient!, compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37d103dd-a2f9-4547-91b6-6e563aff2fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11.131713 seconds (111.12 M allocations: 4.231 GiB, 29.07% gc time, 13.24% compilation time)\n",
      "  4.690610 seconds (250.37 k allocations: 16.853 MiB, 6.35% compilation time)\n",
      "  4.336510 seconds\n"
     ]
    }
   ],
   "source": [
    "### This is following an ReverseDiff.jl example\n",
    "\n",
    "# some inputs and work buffer to play around with\n",
    "grad_array = similar(thetas);\n",
    "\n",
    "# pre-record a GradientTape for `gradsimulation` using inputs of length m with Float64 elements\n",
    "@time const simulation_tape = GradientTape(lossfunction, thetas)\n",
    "\n",
    "# first evaluation compiles and is slower\n",
    "@time gradient!(grad_array, simulation_tape, thetas)\n",
    "# second evaluation\n",
    "@time gradient!(grad_array, simulation_tape, thetas);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "789942a9-ad5a-4e77-b53d-768fd326a8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28.224760 seconds (151.30 M allocations: 6.198 GiB, 53.10% gc time, 1.49% compilation time)\n",
      "  3.261212 seconds (43.11 k allocations: 2.862 MiB, 2.30% compilation time)\n",
      "  3.295330 seconds\n"
     ]
    }
   ],
   "source": [
    "# compile to make it even faster\n",
    "@time const compiled_simulation_tape = compile(simulation_tape)\n",
    "\n",
    "# first evaluation compiles and is slower\n",
    "@time gradient!(grad_array, compiled_simulation_tape, thetas)\n",
    "# second evaluation\n",
    "@time gradient!(grad_array, compiled_simulation_tape, thetas);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36437c88-7dc7-4384-bcf2-60c02d78638c",
   "metadata": {},
   "source": [
    "`grad_array` here carries the gradient result. It is changed in-place in `gradient!` so that the array does not need to get allocated over and over.\n",
    "\n",
    "See how calculating the gradient is only a few times slower than calculating the loss! The magic if reverse-mode differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12fda347-d22c-4739-9c42-4b76365d184a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508-element Vector{Float64}:\n",
       "  0.1853732642158934\n",
       "  0.19537029860429064\n",
       "  0.30751296953662327\n",
       "  0.9695704885915633\n",
       " -0.021084983260654735\n",
       "  0.6163209188019104\n",
       "  0.9534016924800559\n",
       " -1.1217238397902463\n",
       "  1.0582131923675355\n",
       " -0.599928157229062\n",
       "  ⋮\n",
       " -0.6755773301482302\n",
       "  0.3359915238950176\n",
       " -0.03296310450242135\n",
       " -0.015475732504339073\n",
       "  0.38865941344257704\n",
       " -0.44246991175879197\n",
       " -0.372145658742357\n",
       " -0.2178728223760708\n",
       "  0.29274707160701796"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grad_array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
